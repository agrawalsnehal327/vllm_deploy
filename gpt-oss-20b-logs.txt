INFO 08-28 03:16:44 [__init__.py:241] Automatically detected platform cuda.
[1;36m(APIServer pid=1)[0;0m INFO 08-28 03:16:47 [api_server.py:1787] vLLM API server version 0.10.2.dev2+gf5635d62e.d20250807
[1;36m(APIServer pid=1)[0;0m INFO 08-28 03:16:47 [utils.py:326] non-default args: {'host': '0.0.0.0', 'model': 'openai/gpt-oss-20b', 'trust_remote_code': True}
[1;36m(APIServer pid=1)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=1)[0;0m INFO 08-28 03:16:56 [config.py:726] Resolved architecture: GptOssForCausalLM
[1;36m(APIServer pid=1)[0;0m 
Parse safetensors files:   0%|          | 0/3 [00:00<?, ?it/s]
Parse safetensors files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  3.96it/s]
Parse safetensors files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 10.01it/s]
[1;36m(APIServer pid=1)[0;0m INFO 08-28 03:16:57 [config.py:1759] Using max model len 131072
[1;36m(APIServer pid=1)[0;0m WARNING 08-28 03:16:58 [config.py:1198] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[1;36m(APIServer pid=1)[0;0m INFO 08-28 03:16:59 [config.py:2588] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(APIServer pid=1)[0;0m INFO 08-28 03:16:59 [config.py:244] Overriding cuda graph sizes to [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024]
INFO 08-28 03:17:08 [__init__.py:241] Automatically detected platform cuda.
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:10 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:10 [core.py:73] Initializing a V1 LLM engine (v0.10.2.dev2+gf5635d62e.d20250807) with config: model='openai/gpt-oss-20b', speculative_config=None, tokenizer='openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend='openai'), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=openai/gpt-oss-20b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[1024,1008,992,976,960,944,928,912,896,880,864,848,832,816,800,784,768,752,736,720,704,688,672,656,640,624,608,592,576,560,544,528,512,496,480,464,448,432,416,400,384,368,352,336,320,304,288,272,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":1024,"local_cache_dir":null}
[W828 03:17:12.230982027 ProcessGroupNCCL.cpp:915] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_0 pid=311)[0;0m 
[1;36m(EngineCore_0 pid=311)[0;0m              LL          LL          MMM       MMM 
[1;36m(EngineCore_0 pid=311)[0;0m              LL          LL          MMMM     MMMM
[1;36m(EngineCore_0 pid=311)[0;0m          V   LL          LL          MM MM   MM MM
[1;36m(EngineCore_0 pid=311)[0;0m vvvv  VVVV   LL          LL          MM  MM MM  MM
[1;36m(EngineCore_0 pid=311)[0;0m vvvv VVVV    LL          LL          MM   MMM   MM
[1;36m(EngineCore_0 pid=311)[0;0m  vvv VVVV    LL          LL          MM    M    MM
[1;36m(EngineCore_0 pid=311)[0;0m   vvVVVV     LL          LL          MM         MM
[1;36m(EngineCore_0 pid=311)[0;0m     VVVV     LLLLLLLLLL  LLLLLLLLL   M           M
[1;36m(EngineCore_0 pid=311)[0;0m 
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:12 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:12 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:12 [gpu_model_runner.py:1913] Starting to load model openai/gpt-oss-20b...
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:12 [gpu_model_runner.py:1945] Loading model from scratch...
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:12 [cuda.py:323] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=311)[0;0m INFO 08-28 03:17:13 [weight_utils.py:296] Using model weights format ['*.safetensors']
